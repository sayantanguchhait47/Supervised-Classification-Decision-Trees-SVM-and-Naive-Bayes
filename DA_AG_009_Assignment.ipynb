{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "26d987c8",
      "metadata": {
        "id": "26d987c8"
      },
      "source": [
        "# DA-AG-009 Assignment\n",
        "## Supervised Classification: Decision Trees, SVM, and Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c259282",
      "metadata": {
        "id": "6c259282"
      },
      "source": [
        "### Question 1: What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "Information Gain is a metric used to measure how well a feature separates the training examples according to their target classification. It is based on the concept of entropy from information theory. In decision trees, Information Gain is calculated as the reduction in entropy after a dataset is split on a particular feature. The feature with the highest Information Gain is chosen for splitting because it provides the most effective separation of data, leading to purer child nodes and improved classification performance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fde89ed",
      "metadata": {
        "id": "0fde89ed"
      },
      "source": [
        "### Question 2: Difference between Gini Impurity and Entropy\n",
        "\n",
        "Gini Impurity and Entropy are both measures used to evaluate the quality of splits in decision trees. Gini Impurity measures the probability of incorrect classification of a randomly chosen element, while Entropy measures the level of disorder in the dataset. Gini is computationally faster and often used in CART algorithms, whereas Entropy is more informative and based on information theory, commonly used in ID3 and C4.5 algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2506abd7",
      "metadata": {
        "id": "2506abd7"
      },
      "source": [
        "### Question 3: What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "Pre-pruning is a technique used to stop the growth of a decision tree early to prevent overfitting. It involves setting constraints such as maximum tree depth, minimum samples per split, or minimum impurity decrease. By limiting tree complexity during training, pre-pruning improves generalization performance on unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ca96e8c",
      "metadata": {
        "id": "1ca96e8c"
      },
      "source": [
        "### Question 4: Decision Tree using Gini Impurity (Practical)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b29767ae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b29767ae",
        "outputId": "ebfaa6c1-3ba7-4b5f-b635-536f3b686329"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.02666667, 0.        , 0.55072262, 0.42261071])"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "model = DecisionTreeClassifier(criterion='gini')\n",
        "model.fit(X, y)\n",
        "\n",
        "model.feature_importances_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "694103d4",
      "metadata": {
        "id": "694103d4"
      },
      "source": [
        "### Question 5: What is a Support Vector Machine (SVM)?\n",
        "\n",
        "A Support Vector Machine is a supervised learning algorithm used for classification and regression tasks. It works by finding the optimal hyperplane that maximally separates data points of different classes. SVM focuses on boundary data points known as support vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "907c0a93",
      "metadata": {
        "id": "907c0a93"
      },
      "source": [
        "### Question 6: What is the Kernel Trick in SVM?\n",
        "\n",
        "The Kernel Trick allows SVMs to classify non-linearly separable data by transforming it into a higher-dimensional space. Kernels such as linear, polynomial, and radial basis function (RBF) compute inner products in this space without explicit transformation, reducing computational cost."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7994d8d",
      "metadata": {
        "id": "c7994d8d"
      },
      "source": [
        "### Question 7: SVM with Linear and RBF Kernels (Practical)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7059125a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7059125a",
        "outputId": "4ac2deb4-731f-4c23-8e60-ac8d5c68e141"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9814814814814815, 0.7592592592592593)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_wine(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "\n",
        "svm_linear.fit(X_train, y_train)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "acc_linear = accuracy_score(y_test, svm_linear.predict(X_test))\n",
        "acc_rbf = accuracy_score(y_test, svm_rbf.predict(X_test))\n",
        "\n",
        "acc_linear, acc_rbf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5020d26",
      "metadata": {
        "id": "e5020d26"
      },
      "source": [
        "### Question 8: What is the Naïve Bayes classifier, and why is it called 'Naïve'?\n",
        "\n",
        "Naïve Bayes is a probabilistic classifier based on Bayes’ Theorem. It is called 'Naïve' because it assumes that all features are independent of each other given the class label, which is rarely true in real-world data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d50cebf1",
      "metadata": {
        "id": "d50cebf1"
      },
      "source": [
        "### Question 9: Differences between Gaussian, Multinomial, and Bernoulli Naïve Bayes\n",
        "\n",
        "Gaussian Naïve Bayes is used for continuous data assuming normal distribution. Multinomial Naïve Bayes is suitable for discrete counts such as text data. Bernoulli Naïve Bayes works with binary features and is commonly applied in document classification."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a6c1020",
      "metadata": {
        "id": "6a6c1020"
      },
      "source": [
        "### Question 10: Gaussian Naïve Bayes on Breast Cancer Dataset (Practical)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9287228a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9287228a",
        "outputId": "deda0ba6-b35c-4d0c-8f98-dcaa41c812f5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9415204678362573"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "accuracy_score(y_test, gnb.predict(X_test))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}